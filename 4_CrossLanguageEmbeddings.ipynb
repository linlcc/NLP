{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_CrossLanguageEmbeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linlcc/NLP/blob/master/4_CrossLanguageEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDsn_Motja8Q",
        "colab_type": "text"
      },
      "source": [
        "# Cross-Language Word Embeddings\n",
        "\n",
        "Different modeling choices for word embeddings may be ultimately evaluated by the effectiveness of classifiers, parsers, and other inference models that use those embeddings.\n",
        "\n",
        "In this, however, we will consider another common method of evaluating word embeddings: by judging the usefulness of pairwise distances between words in the embedding space.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKm5cPMQ2xHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.word2vec import LineSentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfKjYFDklB4c",
        "colab_type": "text"
      },
      "source": [
        "We'll start by downloading a plain-text version of the Shakespeare plays we used for the first assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw3bvl1yf5FB",
        "colab_type": "code",
        "outputId": "934e9334-df53-4d84-fe73-dd4ea31c1356",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/shakespeare_plays.txt\n",
        "lines = [s.split() for s in open('shakespeare_plays.txt')]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-15 20:16:50--  http://www.ccs.neu.edu/home/dasmith/courses/cs6120/shakespeare_plays.txt\n",
            "Resolving www.ccs.neu.edu (www.ccs.neu.edu)... 52.70.229.197\n",
            "Connecting to www.ccs.neu.edu (www.ccs.neu.edu)|52.70.229.197|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4746840 (4.5M) [text/plain]\n",
            "Saving to: ‘shakespeare_plays.txt.1’\n",
            "\n",
            "shakespeare_plays.t 100%[===================>]   4.53M  19.0MB/s    in 0.2s    \n",
            "\n",
            "2020-04-15 20:16:50 (19.0 MB/s) - ‘shakespeare_plays.txt.1’ saved [4746840/4746840]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cZ52pEflKKM",
        "colab_type": "text"
      },
      "source": [
        "Then, we'll estimate a simple word2vec model on the Shakespeare texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXT5BNPs_zjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Word2Vec(lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzt3lG1-lw33",
        "colab_type": "text"
      },
      "source": [
        "Even with such a small training set size, I can perform some standard analogy tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4ruAqhKC3-R",
        "colab_type": "code",
        "outputId": "e63d50e2-d908-4192-8f12-1da7b24de53a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "model.wv.most_similar(positive=['king','woman'], negative=['man'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.8442368507385254),\n",
              " ('prince', 0.7322368621826172),\n",
              " ('warwick', 0.7289110422134399),\n",
              " ('duke', 0.7008894681930542),\n",
              " ('margaret', 0.6998493671417236),\n",
              " ('widow', 0.6977822780609131),\n",
              " ('clarence', 0.6935532689094543),\n",
              " ('gloucester', 0.6768079996109009),\n",
              " ('york', 0.6758090257644653),\n",
              " ('elizabeth', 0.664851188659668)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJL45y5emjA9",
        "colab_type": "text"
      },
      "source": [
        "For the rest of this, we will focus on finding words with similar embeddings, both within and across languages. For example, what words are similar to the name of the title character of *Othello*?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EZGroU0KPyj",
        "colab_type": "code",
        "outputId": "20b31684-bc15-4e70-89fd-8633a5782dc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "model.wv.most_similar(positive=['othello'])\n",
        "#model.wv.most_similar(positive=['brutus'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('desdemona', 0.9487350583076477),\n",
              " ('iago', 0.9231839776039124),\n",
              " ('ham', 0.9201522469520569),\n",
              " ('cleopatra', 0.9169795513153076),\n",
              " ('proteus', 0.9152540564537048),\n",
              " ('cressida', 0.9049752950668335),\n",
              " ('anne', 0.9027912616729736),\n",
              " ('valentine', 0.9011837244033813),\n",
              " ('pisanio', 0.9009215831756592),\n",
              " ('troilus', 0.8984599113464355)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM2BT_7zZle3",
        "colab_type": "text"
      },
      "source": [
        "This search uses cosine similarity. In the default API, we could see the same similarity between the words `othello` and `desdemona` as in the search results above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e32-u4zYFda",
        "colab_type": "code",
        "outputId": "bbaf13ad-ee40-4db2-97fe-16f332fa2ec9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "model.wv.similarity('othello', 'desdemona')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9487352"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c49DwfAmZ6PU",
        "colab_type": "text"
      },
      "source": [
        "Implement our own cosine similarity function so that I can reuse it outside of the context of the gensim model object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEj2PqpuZ5xs",
        "colab_type": "code",
        "outputId": "20ec5066-5391-43b7-c19b-8404b8e1902c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Implement cosim\n",
        "def cosim(v1, v2):\n",
        "  ## return cosine similarity between v1 and v2\n",
        "  return (np.dot(v1,v2)/(np.linalg.norm(v1)*np.linalg.norm(v2)))\n",
        "\n",
        "## Should give a result similar to model.wv.similarity:\n",
        "cosim(model.wv['othello'], model.wv['desdemona'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.94873506"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TbDqBIHbHfB",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "We could collect a lot of human judgments about how similar pairs of words, or pairs of Shakespearean characters, are. Then we could compare different word-embedding models by their ability to replicate these human judgments.\n",
        "\n",
        "If we extend our ambition to multiple languages, however, we can use a word translation task to evaluate word embeddings.\n",
        "\n",
        "We will use a subset of [Facebook AI's FastText cross-language embeddings](https://fasttext.cc/docs/en/aligned-vectors.html) for several languages. Your task will be to compare English both to French, and to *one more language* from the following set: Arabic, German, Portuguese, Russian, Spanish, Vietnamese, and Chinese."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC_FXRnfq1BO",
        "colab_type": "code",
        "outputId": "3d1fa116-041c-40b3-f10c-0081de04ea88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "!wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.en.vec\n",
        "!wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.fr.vec\n",
        "\n",
        "# TODO: uncomment at least one of these\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.ar.vec\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.de.vec\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.pt.vec\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.ru.vec\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.es.vec\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.vi.vec\n",
        "!wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.zh.vec"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-15 20:17:13--  http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.en.vec\n",
            "Resolving www.ccs.neu.edu (www.ccs.neu.edu)... 52.70.229.197\n",
            "Connecting to www.ccs.neu.edu (www.ccs.neu.edu)|52.70.229.197|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67681172 (65M)\n",
            "Saving to: ‘30k.en.vec.1’\n",
            "\n",
            "30k.en.vec.1        100%[===================>]  64.54M  74.0MB/s    in 0.9s    \n",
            "\n",
            "2020-04-15 20:17:14 (74.0 MB/s) - ‘30k.en.vec.1’ saved [67681172/67681172]\n",
            "\n",
            "--2020-04-15 20:17:15--  http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.fr.vec\n",
            "Resolving www.ccs.neu.edu (www.ccs.neu.edu)... 52.70.229.197\n",
            "Connecting to www.ccs.neu.edu (www.ccs.neu.edu)|52.70.229.197|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67802327 (65M)\n",
            "Saving to: ‘30k.fr.vec.1’\n",
            "\n",
            "30k.fr.vec.1        100%[===================>]  64.66M  65.2MB/s    in 1.0s    \n",
            "\n",
            "2020-04-15 20:17:16 (65.2 MB/s) - ‘30k.fr.vec.1’ saved [67802327/67802327]\n",
            "\n",
            "--2020-04-15 20:17:16--  http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.zh.vec\n",
            "Resolving www.ccs.neu.edu (www.ccs.neu.edu)... 52.70.229.197\n",
            "Connecting to www.ccs.neu.edu (www.ccs.neu.edu)|52.70.229.197|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67627343 (64M)\n",
            "Saving to: ‘30k.zh.vec.1’\n",
            "\n",
            "30k.zh.vec.1        100%[===================>]  64.49M  66.8MB/s    in 1.0s    \n",
            "\n",
            "2020-04-15 20:17:18 (66.8 MB/s) - ‘30k.zh.vec.1’ saved [67627343/67627343]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmuIvGpNrJPe",
        "colab_type": "text"
      },
      "source": [
        "We'll start by loading the word vectors from their textual file format to a dictionary mapping words to numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbWORXkP2Vvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vecref(s):\n",
        "  (word, srec) = s.split(' ', 1)\n",
        "  return (word, np.fromstring(srec, sep=' '))\n",
        "\n",
        "def ftvectors(fname):\n",
        "  return { k:v for (k, v) in [vecref(s) for s in open(fname)] if len(v) > 1} \n",
        "\n",
        "envec = ftvectors('30k.en.vec')\n",
        "frvec = ftvectors('30k.fr.vec')\n",
        "\n",
        "# load vectors for one more language, such as zhvec\n",
        "# arvec = ftvectors('30k.ar.vec')\n",
        "# devec = ftvectors('30k.de.vec')\n",
        "# ptvec = ftvectors('30k.pt.vec')\n",
        "# ruvec = ftvectors('30k.ru.vec')\n",
        "# esvec = ftvectors('30k.es.vec')\n",
        "# vivec = ftvectors('30k.vi.vec')\n",
        "zhvec = ftvectors('30k.zh.vec')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j88E1JdueZHc",
        "colab_type": "text"
      },
      "source": [
        "Write a simple function that takes a vector and a dictionary of vectors and finds the most similar item in the dictionary. For this, I use a linear scan through the dictionary using your `cosim` function from above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmdirYOjoSWV",
        "colab_type": "code",
        "outputId": "a9c4180f-af33-4961-9720-722d072c7015",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "## TODO: implement this search function\n",
        "def mostSimilar(vec, vecDict):\n",
        "  ## Use your cosim function from above\n",
        "  re_dict={}\n",
        "  for item in vecDict:\n",
        "    a=vecDict[item]\n",
        "    re=cosim(vec,a)\n",
        "    re_dict[item]=re\n",
        "  dist_sort=sorted(re_dict.items(), key=lambda dist: dist[1],reverse = True)\n",
        "  mostSimilar = dist_sort[0][0]\n",
        "  similarity = dist_sort[0][1]\n",
        "\n",
        "  return (mostSimilar, similarity)\n",
        "\n",
        "## some example searches\n",
        "[mostSimilar(envec[e], frvec) for e in ['computer', 'germany', 'matrix', 'physics', 'yeast']]\n",
        "# [mostSimilar(envec[e], zhvec) for e in ['computer', 'germany', 'matrix', 'physics', 'yeast']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('informatique', 0.5023827767603765),\n",
              " ('allemagne', 0.593718413875964),\n",
              " ('matrice', 0.5088361302065517),\n",
              " ('physique', 0.4555543434796394),\n",
              " ('fermentation', 0.3504105196166514)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIKUD5qxpUMB",
        "colab_type": "text"
      },
      "source": [
        "Some matches make more sense than others. Note that `computer` most closely matches `informatique`, the French term for *computer science*. If we looked further down the list, we would see `ordinateur`, the term for *computer*. This is one weakness of a focus only on embeddings for word *types* independent of context.\n",
        "\n",
        "To evalute cross-language embeddings more broadly, we'll look at a dataset of links between Wikipedia articles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az10sIFwsEUP",
        "colab_type": "code",
        "outputId": "5acb51bb-ee93-4482-d232-530307063c8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/links.tab\n",
        "links = [s.split() for s in open('links.tab')]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-15 20:17:47--  http://www.ccs.neu.edu/home/dasmith/courses/cs6120/links.tab\n",
            "Resolving www.ccs.neu.edu (www.ccs.neu.edu)... 52.70.229.197\n",
            "Connecting to www.ccs.neu.edu (www.ccs.neu.edu)|52.70.229.197|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1408915 (1.3M)\n",
            "Saving to: ‘links.tab.1’\n",
            "\n",
            "\rlinks.tab.1           0%[                    ]       0  --.-KB/s               \rlinks.tab.1         100%[===================>]   1.34M  7.71MB/s    in 0.2s    \n",
            "\n",
            "2020-04-15 20:17:47 (7.71 MB/s) - ‘links.tab.1’ saved [1408915/1408915]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqHq0hFCv8NY",
        "colab_type": "text"
      },
      "source": [
        "This `links` variable consists of triples of `(English term, language, term in that language)`. For example, here is the link between English `academy` and French `académie`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ7eusdxtdsq",
        "colab_type": "code",
        "outputId": "ce002a0f-6635-49f3-a348-c5ed88a9809e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "links[302]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['academy', 'fr', 'académie']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYEdOQbmwql3",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the English and French embeddings by computing the proportion of English Wikipedia articles whose corresponding French article is also the closest word in embedding space. Skip English articles not covered by the word embedding dictionary. Since many articles, e.g., about named entities have the same title in English and French, compute the baseline accuracy achieved by simply echoing the English title as if it were French. Remember to iterate only over English Wikipedia articles, not the entire embedding dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20xFhkVVGtxH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def norm_vec(vecDict):\n",
        "  mod_vec={}\n",
        "  for item in vecDict:\n",
        "    mod_vec[item]=np.linalg.norm(vecDict[item])\n",
        "  return mod_vec\n",
        "\n",
        "def cosim2(v1, v2, v3, v4):\n",
        "  return (np.dot(v1,v2)/(v3*v4))\n",
        "\n",
        "def mostSimilar2(vec,normvec, vecDict,normvecdict):\n",
        "  re_dict={}\n",
        "  for item in vecDict:\n",
        "    a=vecDict[item]\n",
        "    b=normvecdict[item]\n",
        "    re=cosim2(vec,a,normvec,b)\n",
        "    re_dict[item]=re\n",
        "  dist_sort=sorted(re_dict.items(), key=lambda dist: dist[1],reverse = True)\n",
        "  mostSimilar = dist_sort[0][0]\n",
        "  similarity = dist_sort[0][1]\n",
        "  return (mostSimilar, similarity)\n",
        "\n",
        "# ## some example searches\n",
        "# norm_envec=norm_vec(envec)\n",
        "# norm_frvec=norm_vec(frvec)\n",
        "# [mostSimilar2(envec[e],norm_envec[e],frvec,norm_frvec) for e in ['computer', 'germany', 'matrix', 'physics', 'yeast']]\n",
        "\n",
        "def evaluate(vecdict1,vecdict2,targetvec):\n",
        "  norm_vecdict1=norm_vec(vecdict1)\n",
        "  norm_vecdict2=norm_vec(vecdict2)\n",
        "  i=0\n",
        "  j=0\n",
        "  k=0\n",
        "  for item in links:\n",
        "    if item[1]==targetvec:\n",
        "      i=i+1\n",
        "      if item[0]==item[2]:\n",
        "        k=k+1\n",
        "      a=mostSimilar2(vecdict1[item[0]],norm_vecdict1[item[0]],vecdict2,norm_vecdict2)\n",
        "      if a[0]==item[2]:\n",
        "        j=j+1\n",
        "  return {'accuracy': j/i,\n",
        "          'baselineAccuracy':k/i}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G-9VII70FZx",
        "colab_type": "code",
        "outputId": "4d6cb121-08b2-419c-b626-042846422722",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "evaluate(envec,frvec,'fr')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.5359205593271862, 'baselineAccuracy': 0.6742324450298915}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hqd1buq-OEo",
        "colab_type": "text"
      },
      "source": [
        "Compute accuracy and baseline (identity function) acccuracy for Englsih and another language besides French. Although the baseline will be lower for languages not written in the Roman alphabet (i.e., Arabic or Chinese), there are still many articles in those languages with headwords written in Roman characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjnKtHya-jmj",
        "colab_type": "code",
        "outputId": "33c50d0c-def4-458d-b87f-40681e59b1e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Compute English-X Wikipedia retrieval accuracy.\n",
        "evaluate(envec,zhvec,'zh')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.13639637044505257, 'baselineAccuracy': 0.06740602045225406}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    }
  ]
}