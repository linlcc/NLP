{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.1_LanguageModeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linlcc/NLP/blob/master/2_1_LanguageModeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moWB9udaKesP",
        "colab_type": "text"
      },
      "source": [
        "#LanguageModeling\n",
        "\n",
        "In this task, we will train *character-level* language models. \n",
        "You will train unigram, bigram, and trigram character level models on a collection of books from Project Gutenberg. You will then use these trained English language models to distinguish English documents from Brazilian Portuguese documents in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHFJmuftHJld",
        "colab_type": "code",
        "outputId": "c0dffb9b-679a-4d90-8d6c-a3157ce045f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import httpimport\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "with httpimport.remote_repo(['lm_helper'], 'https://raw.githubusercontent.com/jasoriya/CS6120-PS2-support/master/utils/'):\n",
        "  from lm_helper import get_train_data, get_test_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/mac_morpho.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U0UCuyHQkai",
        "colab_type": "text"
      },
      "source": [
        "This code loads the training and test data. Each dataset is a list of books. Each book contains a list of sentences, and each sentence contains a list of words. For building a character language model, you should join the words of a sentence together with a space character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x0pfuiEChTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the train and test data\n",
        "train = get_train_data()\n",
        "test, test_files = get_test_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-Tj7z5Q-HeG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare all the contents formated properly for the later use \n",
        "\n",
        "test_sec=[]\n",
        "for item in test:\n",
        "    test1=[]    \n",
        "    for i in item:\n",
        "        test1.append(' '.join(word for word in i))        \n",
        "    test_sec.append(test1)\n",
        "    \n",
        "con=[]\n",
        "corpus=[]\n",
        "for i in train:\n",
        "    for item in i:\n",
        "        con.append(' '.join(word for word in item))\n",
        "corpus.append(' '.join(con))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "training, held_out = train_test_split(con, test_size=0.2) ##training set (80%) and a held-out (20%)\n",
        "training80=[]\n",
        "training80.append(' '.join(training))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WAO9VjFLArq",
        "colab_type": "text"
      },
      "source": [
        "## 1.1\n",
        "Collect statistics on the unigram, bigram, and trigram character counts.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh4VOoiSIoUF",
        "colab_type": "code",
        "outputId": "6e90c81f-4d38-46e6-d77a-29cfd9e7085c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "count_vect = CountVectorizer(analyzer='char', ngram_range=(1, 1))\n",
        "texts = count_vect.fit_transform(corpus) # creat sparse matrix of contect list\n",
        "vocab=count_vect.get_feature_names() # list of the vocabularies\n",
        "\n",
        "unigram=texts.toarray()\n",
        "unigram_counts = dict(zip(vocab, unigram[0]))\n",
        "# print(unigram_counts)\n",
        "def keyfunction(k):\n",
        "    return unigram_counts[k]\n",
        "print('Top 10 character counts in unigram: ')\n",
        "for key in sorted(unigram_counts, key=keyfunction, reverse=True)[:10]:\n",
        "    print (\"%s: %i\" % (key, unigram_counts[key]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 character counts in unigram: \n",
            " : 2621784\n",
            "e: 1119617\n",
            "t: 827161\n",
            "a: 731203\n",
            "o: 678136\n",
            "h: 650743\n",
            "n: 615091\n",
            "i: 577691\n",
            "s: 556863\n",
            "r: 502402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixbepTju-Qu8",
        "colab_type": "code",
        "outputId": "e059fff1-66d2-4596-b7ee-11680a5b957b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "count_vect = CountVectorizer(analyzer='char', ngram_range=(2, 2))\n",
        "texts = count_vect.fit_transform(corpus) # creat sparse matrix of contect list\n",
        "vocab=count_vect.get_feature_names() # list of the vocabularies\n",
        "\n",
        "bigram=texts.toarray()\n",
        "bigram_counts = dict(zip(vocab, bigram[0]))\n",
        "# print(bigram_counts)\n",
        "def keyfunction(k):\n",
        "    return bigram_counts[k]\n",
        "print('Top 10 character counts in bigram: ')\n",
        "for key in sorted(bigram_counts, key=keyfunction, reverse=True)[:10]:\n",
        "    print (\"%s: %i\" % (key, bigram_counts[key]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 character counts in bigram: \n",
            "e : 428723\n",
            " t: 354340\n",
            "th: 330682\n",
            "he: 291389\n",
            "d : 268321\n",
            " a: 245575\n",
            "t : 217685\n",
            "s : 215867\n",
            " ,: 192106\n",
            ", : 186347\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFwQN9rG-VKQ",
        "colab_type": "code",
        "outputId": "4a99e4e0-4af8-42ea-a1be-805f86bfa06d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "count_vect = CountVectorizer(analyzer='char', ngram_range=(3, 3))\n",
        "texts = count_vect.fit_transform(corpus) # creat sparse matrix of contect list\n",
        "vocab=count_vect.get_feature_names() # list of the vocabularies\n",
        "\n",
        "trigram=texts.toarray()\n",
        "trigram_counts = dict(zip(vocab, trigram[0]))\n",
        "# print(trigram_counts)\n",
        "def keyfunction(k):\n",
        "    return trigram_counts[k]\n",
        "print('Top 10 character counts in trigram: ')\n",
        "for key in sorted(trigram_counts, key=keyfunction, reverse=True)[:10]:\n",
        "    print (\"%s: %i\" % (key, trigram_counts[key]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 character counts in trigram: \n",
            " th: 263231\n",
            "the: 205546\n",
            " , : 186091\n",
            "he : 168863\n",
            "nd : 115035\n",
            " an: 110442\n",
            "and: 108955\n",
            " of: 75678\n",
            " . : 73780\n",
            "of : 72522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS3mnaIvQnhI",
        "colab_type": "text"
      },
      "source": [
        "## 1.2\n",
        "Calculate the perplexity for each document in the test set using linear interpolation smoothing method. For determining λs for linear interpolation, you can divide the training data into a new training set (80%) and a held-out set (20%), then using grid search method:\n",
        "Choose ~10 values of λ to test using grid search on held-out data.\n",
        "\n",
        "Some documents in the test set are in Brazilian Portuguese. Identify them as follows: \n",
        "  - Sort by perplexity and set a cut-off threshold. All the documents above this threshold score should be categorized as Brazilian Portuguese. \n",
        "  - Print the file names (from `test_files`) and perplexities of the documents above the threshold\n",
        "\n",
        "    ```\n",
        "        file name, score\n",
        "        file name, score\n",
        "        . . .\n",
        "        file name, score\n",
        "    ```\n",
        "\n",
        "  - Copy this list of filenames and manually annotate them as being correctly or incorrectly labeled as Portuguese.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQF4HhQGOZD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ngram(x,y,z): #x=ngram, y=trainfile, z=lambda\n",
        "    count_vect = CountVectorizer(analyzer='char', ngram_range=(x, x),token_pattern=r'\\b[^\\d\\W]+\\b')\n",
        "    texts = count_vect.fit_transform(y) # creat sparse matrix of contect list\n",
        "    vocab=count_vect.get_feature_names() # list of the vocabularies\n",
        "\n",
        "    trigram=texts.toarray()\n",
        "    trigram_counts = dict(zip(vocab, trigram[0]))\n",
        "\n",
        "    trigram_counts = {k: (v+z) / (total+len(vocab)*z) for total in (sum(trigram_counts.values()),) for k, v in trigram_counts.items()}\n",
        "    return trigram_counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg4SFYbV-eQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unigram_counts=ngram(1,training80,0)\n",
        "bigram_counts=ngram(2,training80,0)\n",
        "trigram_counts=ngram(3,training80,0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOmBJRDJ-eU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getvalue(x,y):\n",
        "    a=x.get(y)\n",
        "    if a is not None:\n",
        "        return a\n",
        "    else:\n",
        "        return 10e-10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2D0c_Kf-eXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perplexity_try(content,l1,l2,l3):\n",
        "    total=0\n",
        "    count=0    \n",
        "    \n",
        "    for j in content:\n",
        "        count=count + len(j)\n",
        "\n",
        "        for i in range(len(j)):\n",
        "            if i==0:\n",
        "                a=getvalue(unigram_counts,(j[i].lower()))\n",
        "                b=10e-10\n",
        "                c=10e-10\n",
        "            elif i==1:    \n",
        "                a=getvalue(unigram_counts,(j[i].lower()))\n",
        "                b=getvalue(bigram_counts,(j[i-1].lower()+j[i].lower()))\n",
        "                c=10e-10\n",
        "            else:    \n",
        "                a=getvalue(unigram_counts,(j[i].lower()))     \n",
        "                b=getvalue(bigram_counts,(j[i-1].lower()+j[i].lower()))\n",
        "                c=getvalue(trigram_counts,(j[i-2].lower()+j[i-1].lower()+j[i].lower()))\n",
        "            \n",
        "            k=l1*a+l2*b+l3*c\n",
        "            total=total+math.log(k,2)     \n",
        "            \n",
        "    perplexity=pow(2,((-1/count)*total))\n",
        "\n",
        "    return perplexity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljKi57Qb-ecU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 10 values of λ\n",
        "lset=[[0.8,0.1,0.1],[0.7,0.2,0.1],[0.6,0.2,0.2],[0.5,0.3,0.2],[0.4,0.5,0.1],\n",
        "      [0.3,0.5,0.2],[0.2,0.6,0.2],[0.2,0.7,0.1],[0.2,0.3,0.5],[0.1,0.3,0.6]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xWtfJVx-pss",
        "colab_type": "code",
        "outputId": "ccc2d6f6-2369-47f3-b86f-f359950d1ae0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# grid search for the best combination of λ\n",
        "\n",
        "for item in lset:\n",
        "    print(item[0],item[1],item[2],perplexity_try(held_out,item[0],item[1],item[2]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8 0.1 0.1 23.071218962945615\n",
            "0.7 0.2 0.1 25.638068577134934\n",
            "0.6 0.2 0.2 29.463385661521066\n",
            "0.5 0.3 0.2 33.855647559976994\n",
            "0.4 0.5 0.1 38.92781795857396\n",
            "0.3 0.5 0.2 48.77551189653424\n",
            "0.2 0.6 0.2 63.351709305282434\n",
            "0.2 0.7 0.1 61.261704630461\n",
            "0.2 0.3 0.5 71.07245938514383\n",
            "0.1 0.3 0.6 114.8418006291888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NFrwrHE-pxn",
        "colab_type": "code",
        "outputId": "c1a4e95b-da27-477b-d5d9-1f5356143789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# calulate perplexities of the documents and show files name which is above the threshold=35\n",
        "\n",
        "i=0\n",
        "thresould=35\n",
        "for item in range(len(test_sec)):\n",
        "    k=perplexity_try(test_sec[item],0.8,0.1,0.1)\n",
        "    if k>=thresould:\n",
        "        i=i+1\n",
        "        print( test_files[item],\"%.2f\" % k)\n",
        "print('Total above the thresould: '+ str(i))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "br94ma01.txt 38.80\n",
            "ag94ja11.txt 40.14\n",
            "ag94ju07.txt 42.60\n",
            "ag94fe1.txt 39.83\n",
            "br94ju01.txt 39.38\n",
            "br94ag01.txt 40.97\n",
            "br94fe1.txt 41.50\n",
            "ag94de06.txt 41.43\n",
            "br94de01.txt 39.29\n",
            "ag94ma03.txt 42.53\n",
            "br94ja04.txt 41.00\n",
            "ag94jl12.txt 41.86\n",
            "br94ab02.txt 38.70\n",
            "br94jl01.txt 39.75\n",
            "ag94no01.txt 42.05\n",
            "ag94ag02.txt 41.98\n",
            "ag94mr1.txt 41.31\n",
            "ag94ou04.txt 40.99\n",
            "ag94ab12.txt 42.85\n",
            "ag94se06.txt 43.39\n",
            "Total above the thresould: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEAvm9_g-p0G",
        "colab_type": "code",
        "outputId": "14f0f96c-7c82-4e24-a61b-52f23c886ee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# manually output all the file names contain \"txt\" which is Portuguese\n",
        "\n",
        "matching=[s for s in test_files if \"txt\" in s]\n",
        "print('Total Portuguese files in test set: '+str(len(matching)))\n",
        "print(matching)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Portuguese files in test set: 20\n",
            "['br94ma01.txt', 'ag94ja11.txt', 'ag94ju07.txt', 'ag94fe1.txt', 'br94ju01.txt', 'br94ag01.txt', 'br94fe1.txt', 'ag94de06.txt', 'br94de01.txt', 'ag94ma03.txt', 'br94ja04.txt', 'ag94jl12.txt', 'br94ab02.txt', 'br94jl01.txt', 'ag94no01.txt', 'ag94ag02.txt', 'ag94mr1.txt', 'ag94ou04.txt', 'ag94ab12.txt', 'ag94se06.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu2T_p9g-4HF",
        "colab_type": "text"
      },
      "source": [
        "Compare the total number of Portuguese files is exactually the same as the files above threshold=35."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQl2u_giVW5e",
        "colab_type": "text"
      },
      "source": [
        "## 1.3\n",
        "Build a trigram language model with add-λ smoothing (use λ = 0.1).\n",
        "\n",
        "Sort the test documents by perplexity and perform a check for Brazilian Portuguese documents as above:\n",
        "\n",
        "  - Observe the perplexity scores and set a cut-off threshold. All the documents above this threshold score should be categorized as Brazilian Portuguese. \n",
        "  - Print the file names and perplexities of the documents above the threshold\n",
        "\n",
        "  ```\n",
        "      file name, score\n",
        "      file name, score\n",
        "      . . .\n",
        "      file name, score\n",
        "  ```\n",
        "\n",
        "  - Copy this list of filenames and manually annotate them for correctness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGUTEk8QUehL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# trigram model with λ = 0.1\n",
        "trigram_counts=ngram(3,training80,0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQy605Za-6Bm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perplexity_tri(content):\n",
        "    total=0\n",
        "    count=0    \n",
        "    \n",
        "    for j in content:\n",
        "        count=count + len(j)\n",
        "        for i in range(2,len(j)):  \n",
        "            c=getvalue(trigram_counts,(j[i-2].lower()+j[i-1].lower()+j[i].lower()))          \n",
        "            total=total+math.log(c,2)     \n",
        "            \n",
        "    perplexity=pow(2,((-1/count)*total))\n",
        "    return perplexity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t7iv3HM-6ED",
        "colab_type": "code",
        "outputId": "61824f39-2707-4d80-fc87-c43bf23b58a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# calulate perplexities of the documents and show files name which is above the threshold=10000\n",
        "i=0\n",
        "thresould=10000\n",
        "for item in range(len(test_sec)):\n",
        "    k=perplexity_tri(test_sec[item])\n",
        "    if k>=thresould:\n",
        "        i=i+1\n",
        "        print( test_files[item],\"%.2f\" % k)\n",
        "print('Total above the thresould: '+ str(i))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "br94ma01.txt 15089.75\n",
            "ag94ja11.txt 15284.46\n",
            "ag94ju07.txt 16987.75\n",
            "ag94fe1.txt 14691.65\n",
            "br94ju01.txt 14197.61\n",
            "br94ag01.txt 15982.04\n",
            "br94fe1.txt 15823.80\n",
            "ag94de06.txt 16451.39\n",
            "br94de01.txt 13676.63\n",
            "ag94ma03.txt 17549.97\n",
            "br94ja04.txt 15493.30\n",
            "ag94jl12.txt 16248.47\n",
            "br94ab02.txt 14081.14\n",
            "br94jl01.txt 14286.58\n",
            "ag94no01.txt 16207.51\n",
            "ag94ag02.txt 16008.43\n",
            "ag94mr1.txt 16239.16\n",
            "ag94ou04.txt 15375.28\n",
            "ag94ab12.txt 16071.63\n",
            "ag94se06.txt 17499.64\n",
            "Total above the thresould: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uObOpW2-6HT",
        "colab_type": "code",
        "outputId": "93bf6622-a02a-463c-ab26-9edbfcfd9d86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# manually output all the file names contain \"txt\" which is Portuguese\n",
        "\n",
        "print('Total Portuguese files in test set: '+str(len(matching)))\n",
        "print(matching)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Portuguese files in test set: 20\n",
            "['br94ma01.txt', 'ag94ja11.txt', 'ag94ju07.txt', 'ag94fe1.txt', 'br94ju01.txt', 'br94ag01.txt', 'br94fe1.txt', 'ag94de06.txt', 'br94de01.txt', 'ag94ma03.txt', 'br94ja04.txt', 'ag94jl12.txt', 'br94ab02.txt', 'br94jl01.txt', 'ag94no01.txt', 'ag94ag02.txt', 'ag94mr1.txt', 'ag94ou04.txt', 'ag94ab12.txt', 'ag94se06.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4ZimNNQ_M2f",
        "colab_type": "text"
      },
      "source": [
        "Compare the total number of Portuguese files is exactually the same as the files above threshold=35."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqhXTB5TXR25",
        "colab_type": "text"
      },
      "source": [
        "## 1.4\n",
        "Based on your observation from above questions, compare linear interpolation and add-λ smoothing by listing out their pros and cons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFq1ECgDI6QG",
        "colab_type": "text"
      },
      "source": [
        "Based on your observation from above questions, compare linear interpolation and add-λ smoothing by listing out their pros and cons.\n",
        "\n",
        "For the linear interpolation method, the overall perplexities are all small(under 50); For add-λ smoothing, the overall perplexities are way higher than the linear interpolation method (over 1000). Although both of the methods can correctly distinguish English documents and Brazilian Portuguese documents, but for lower perplexity in the linear interpolation method indicates the probability distribution is better at predicting the test set. However, since linear interpolation method need to consider unigram, bigram and also trigram, which means it needs to spend more time(at least 3 times) to build and train model than only trigram(add-λ smoothing) method. Therefore, both of them are workable for this test set, but if the documents are more complicated, we need to use linear interpolation method, otherwise we can only use trigram to predict."
      ]
    }
  ]
}